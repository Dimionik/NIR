{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Импорт"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pyswarms as ps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_GTZAN = f'D:\\\\GTZAN\\\\Data'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# genres = \"blues classical country disco hiphop jazz metal pop reggae rock\".split()\n",
    "genres = \"blues classical country disco\".split()\n",
    "genres_arr = np.asarray(genres)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_of_genres_and_compositions_in_genre = []\n",
    "for genre in genres_arr:\n",
    "    # dir_path = f'{path_to_GTZAN}\\\\genres_original\\\\'+genre\n",
    "    dir_path = f'{path_to_GTZAN}\\\\genres\\\\'+genre\n",
    "    count = 0\n",
    "    for path in os.listdir(dir_path):\n",
    "        if os.path.isfile(os.path.join(dir_path, path)):\n",
    "            count += 1\n",
    "    count_of_genres_and_compositions_in_genre.append([genre, count])\n",
    "    print('In genre', genre, 'have', count, 'compositions')\n",
    "count_of_genres_and_compositions_in_genre"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for genre in genres_arr:\n",
    "    if genre == genres_arr[0]:\n",
    "        audio_data = {}\n",
    "        audio_sr = {}\n",
    "        # dir_path = f'{path_to_GTZAN}\\\\genres_original\\\\'+genre\n",
    "        dir_path = f'{path_to_GTZAN}\\\\genres\\\\'+genre\n",
    "        genre_arr = []\n",
    "        sr_app = []\n",
    "        key_arr = []\n",
    "        for path in os.listdir(dir_path):\n",
    "            AUDIO_FILE = f'{dir_path}\\\\' + path\n",
    "            samples, sample_rate = librosa.load(AUDIO_FILE, mono=False, sr=None)\n",
    "            genre_arr.append(samples)\n",
    "            sr_app.append(sample_rate)\n",
    "            key = path.split(\".\")\n",
    "            key_arr.append(key[1])\n",
    "        index_key = []\n",
    "        for i in key_arr:\n",
    "            i = int(i)\n",
    "            index_key.append(i)\n",
    "        audio_data.update({f'{genre}': genre_arr})\n",
    "        audio_sr.update({f'{genre}-sample_rate': sr_app})\n",
    "        db_genres = pd.DataFrame(audio_data, index=index_key)\n",
    "        db_sr = pd.DataFrame(audio_sr, index=index_key)\n",
    "    elif genre != genres_arr[0]:\n",
    "        audio_data = {}\n",
    "        audio_sr = {}\n",
    "        # dir_path = f'{path_to_GTZAN}\\\\genres_original\\\\'+genre\n",
    "        dir_path = f'{path_to_GTZAN}\\\\genres\\\\'+genre\n",
    "        genre_arr = []\n",
    "        sr_app = []\n",
    "        key_arr = []\n",
    "        for path in os.listdir(dir_path):\n",
    "            AUDIO_FILE = f'{dir_path}\\\\' + path\n",
    "            samples, sample_rate = librosa.load(AUDIO_FILE, mono=False, sr=None)\n",
    "            genre_arr.append(samples)\n",
    "            sr_app.append(sample_rate)\n",
    "            key = path.split(\".\")\n",
    "            key_arr.append(key[1])\n",
    "        index_key = []\n",
    "        for i in key_arr:\n",
    "            i = int(i)\n",
    "            index_key.append(i)\n",
    "        audio_data.update({f'{genre}': genre_arr})\n",
    "        audio_sr.update({f'{genre}-sample_rate': sr_app})\n",
    "        db2 = pd.DataFrame(audio_data, index=index_key)\n",
    "        db_2_sr = pd.DataFrame(audio_sr, index=index_key)\n",
    "        db_genres = db_genres.join(db2)\n",
    "        db_sr = db_sr.join(db_2_sr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db_genres"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db_sr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Проверка длины композиции и подрезка композиций длиннее"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lowest_compose = 0\n",
    "for genre in genres_arr:\n",
    "    for ind in db_genres.index:\n",
    "        compose = db_genres[genre][ind]\n",
    "        len_compose = len(compose)\n",
    "        if lowest_compose == 0:\n",
    "            lowest_compose = len_compose\n",
    "        elif len_compose < lowest_compose:\n",
    "            lowest_compose = len_compose"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for genre in genres_arr:\n",
    "    for ind in db_genres.index:\n",
    "        compose = db_genres[genre][ind]\n",
    "        len_compose = len(compose)\n",
    "        if len_compose > lowest_compose:\n",
    "            db_genres[genre][ind] = compose[:-(len_compose - lowest_compose)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Проверка для галочки"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lenlen = {}\n",
    "# for genre in genres_arr:\n",
    "#     len_array = []\n",
    "#     len_x = 0\n",
    "#     for ind in db_genres.index:\n",
    "#         compose = db_genres[genre][ind]\n",
    "#         len_compose = len(compose)\n",
    "#         if len_compose != len_x:\n",
    "#             len_array.append(len_compose)\n",
    "#             len_x = len_compose\n",
    "#     lenlen.update({genre: len_array})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lenlen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Получение БД спектрограмм"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('cool')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mel_data = {}\n",
    "path_to_spec = f'{path_to_GTZAN}\\\\genres_spec'\n",
    "# path_to_spec = f'{path_to_GTZAN}\\\\genres_original_spec'\n",
    "os.makedirs(path_to_spec, exist_ok=True)\n",
    "for i, j in zip(db_genres.itertuples(), db_sr.itertuples()):\n",
    "    mel_arr = []\n",
    "    for x in range(1, len(i)):\n",
    "        index_sample = i[0]\n",
    "        # print(index_sample)\n",
    "        sample = i[x]\n",
    "        # print(sample)\n",
    "        index_sr = j[0]\n",
    "        sr = j[x]\n",
    "        sgram = librosa.stft(sample)\n",
    "        sgram_mag, sgram_phase = librosa.magphase(sgram)\n",
    "        mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sr)\n",
    "        mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)\n",
    "        mel_arr.append(mel_sgram)\n",
    "        # path_to_genre_spec = f'{path_to_spec}\\\\{genres_arr[x-1]}'\n",
    "        # os.makedirs(path_to_genre_spec, exist_ok=True)\n",
    "        # img = librosa.display.specshow(mel_sgram, sr=sr, x_axis='time', y_axis='mel', cmap=cmap)\n",
    "        # plt.savefig(f\"{path_to_genre_spec}\\\\{genres_arr[x-1]}{index_sample}.jpg\")\n",
    "        # print(f\"Жанр {genres_arr[x-1]} заполнен\")\n",
    "    mel_data.update({f'{i[0]}': mel_arr})\n",
    "    # print(f\"Спектр композиций № {i[0]} сформирован\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = db_genres.columns.to_list()\n",
    "db_mel_sgram = pd.DataFrame(mel_data, index=ax)\n",
    "db_mel_sgram = db_mel_sgram.transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db_genres"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db_mel_sgram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lowest_freq_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Функция вычисления CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CNN(chosen_optimizer, epo, numero_gen=1, preparation=None, plot=None):\n",
    "    # global\n",
    "    print(f'Начало CNN\\n')\n",
    "\n",        
    "    epo = np.int32(epo)\n",
    "    chosen_optimizer = np.int32(chosen_optimizer)\n",
    "\n",
    "    path_to_spec = f'{path_to_GTZAN}\\\\genres_spec'\n",
    "\n",
    "    spectr_height = 256\n",
    "    spectr_width = 256\n",
    "    batch_size = 16\n",
    "    n_channels = 3\n",
    "    n_classes = 10\n",
    "    model_chape = (spectr_height, spectr_width, n_channels)\n",
    "\n",
    "    # Make a dataset containing the training spectrograms\n",
    "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        directory=os.path.join(path_to_spec),\n",
    "        shuffle=True,\n",
    "        color_mode='rgb',\n",
    "        image_size=(spectr_height, spectr_width),\n",
    "        subset=\"training\",\n",
    "        seed=0)\n",
    "\n",
    "    # Make a dataset containing the validation spectrogram\n",
    "    valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        directory=os.path.join(path_to_spec),\n",
    "        shuffle=True,\n",
    "        color_mode='rgb',\n",
    "        image_size=(spectr_height, spectr_width),\n",
    "        subset=\"validation\",\n",
    "        seed=0)\n",
    "\n",
    "    if preparation == True:\n",
    "        def prepare(ds, augment=False):\n",
    "            rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1. / 255)])\n",
    "            flip_and_rotate = tf.keras.Sequential([\n",
    "                tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "            ])\n",
    "\n",
    "            ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "            if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
    "            return ds\n",
    "\n",
    "        train_dataset = prepare(train_dataset, augment=False)\n",
    "        valid_dataset = prepare(valid_dataset, augment=False)\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=model_chape))\n",
    "    model.add(tf.keras.layers.Conv2D(16, 3, strides=2, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    if chosen_optimizer == 1:\n",
    "        optimizer = tf.keras.optimizers.Adadelta()\n",
    "    elif chosen_optimizer == 2:\n",
    "        optimizer = tf.keras.optimizers.Adagrad()\n",
    "    elif chosen_optimizer == 3:\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "    elif chosen_optimizer == 4:\n",
    "        optimizer = tf.keras.optimizers.Adamax()\n",
    "    elif chosen_optimizer == 5:\n",
    "        optimizer = tf.keras.optimizers.Ftrl()\n",
    "    elif chosen_optimizer == 6:\n",
    "        optimizer = tf.keras.optimizers.Nadam()\n",
    "    elif chosen_optimizer == 7:\n",
    "        optimizer = tf.keras.optimizers.RMSprop()\n",
    "    elif chosen_optimizer == 8:\n",
    "        optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=epo,\n",
    "                        validation_data=valid_dataset,\n",
    "                        # verbose=0\n",
    "                        )\n",
    "\n",
    "    history_dict = history.history\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values = history_dict['val_loss']\n",
    "\n",
    "    acc_values = history_dict['accuracy']\n",
    "    val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "    path_to_CNN = f\"{path_to_GTZAN}\\\\CNN\"\n",
    "    os.makedirs(path_to_CNN, exist_ok=True)\n",
    "\n",
    "    if plot == True:\n",
    "        epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "        plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "        plt.title(f'Training and validation loss #{numero_gen}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.savefig(f\"{path_to_CNN}\\\\loss_{numero_gen}.jpg\")\n",
    "\n",
    "        epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(epochs, acc_values, 'bo', label='Training accuracy')\n",
    "        plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n",
    "        plt.title(f'Training and validation accuracy #{numero_gen}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.savefig(f\"{path_to_CNN}\\\\accuracy_{numero_gen}.jpg\")\n",
    "\n",
    "    final_loss, final_acc = model.evaluate(valid_dataset, verbose=0)\n",
    "    print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))\n",
    "\n",
    "    return [loss_values, val_loss_values, final_loss, acc_values, val_acc_values, final_acc]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Запуск основного алгоритма CNN для усреднения результатов без PSO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Запуск алгоритма предсказания CNN на случайных/выбранных настройках\\n')\n",
    "epo = 500\n",
    "n_generation = 10\n",
    "loss_values_arr = []\n",
    "val_loss_values_arr = []\n",
    "final_loss_arr = []\n",
    "acc_values_arr = []\n",
    "val_acc_values_arr = []\n",
    "final_acc_arr = []\n",
    "\n",
    "for numero_gen in range(n_generation):\n",
    "    print(f'==============================\\n'\n",
    "          f'==============================\\n'\n",
    "          f'\\n'\n",
    "          f'GENERATION # {numero_gen}\\n'\n",
    "          f'- - - - - - - - - - - - - - - -\\n')\n",
    "    optimist = [1, 2, 3, 4, 5, 6, 8, 9]\n",
    "    chosen_optimizer = np.random.choice(optimist)\n",
    "    print(f'optimizer = {chosen_optimizer}\\n')\n",
    "    loss_values, val_loss_values, final_loss, acc_values, val_acc_values, final_acc = CNN(chosen_optimizer, epo, numero_gen, preparation=None, plot=True)\n",
    "    loss_values_arr.append(loss_values)\n",
    "    val_loss_values_arr.append(val_loss_values)\n",
    "    final_loss_arr.append(final_loss)\n",
    "    acc_values_arr.append(acc_values)\n",
    "    val_acc_values_arr.append(val_acc_values)\n",
    "    final_acc_arr.append(final_acc)\n",
    "\n",
    "columns = np.arange(start=0,\n",
    "                    stop=epo,\n",
    "                    step=1)\n",
    "\n",
    "db_loss_values = pd.DataFrame(data=loss_values_arr, columns=columns)\n",
    "db_val_loss_values = pd.DataFrame(data=val_loss_values_arr, columns=columns)\n",
    "db_acc_values = pd.DataFrame(data=acc_values_arr, columns=columns)\n",
    "db_val_acc_values = pd.DataFrame(data=val_acc_values_arr, columns=columns)\n",
    "\n",
    "loss_values_mean_arr = []\n",
    "val_loss_values_mean_arr = []\n",
    "acc_values_mean_arr = []\n",
    "val_acc_values_mean_arr = []\n",
    "\n",
    "for i in range(epo):\n",
    "    loss_values_mean = np.mean(db_loss_values[i])\n",
    "    val_loss_values_mean = np.mean(db_val_loss_values[i])\n",
    "    acc_values_mean = np.mean(db_acc_values[i])\n",
    "    val_acc_values_mean = np.mean(db_val_acc_values[i])\n",
    "    loss_values_mean_arr.append(loss_values_mean)\n",
    "    val_loss_values_mean_arr.append(val_loss_values_mean)\n",
    "    acc_values_mean_arr.append(acc_values_mean)\n",
    "    val_acc_values_mean_arr.append(val_acc_values_mean)\n",
    "\n",
    "final_loss_mean = np.mean(final_loss_arr)\n",
    "final_acc_mean = np.mean(final_acc_arr)\n",
    "\n",
    "path_to_CNN = f\"{path_to_GTZAN}\\\\CNN\"\n",
    "os.makedirs(path_to_CNN, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(columns+1, loss_values_mean_arr, 'bo', label='Training loss')\n",
    "plt.plot(columns+1, val_loss_values_mean_arr, 'b', label='Validation loss')\n",
    "plt.title(f'Mean training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f\"{path_to_CNN}\\\\mean_loss.jpg\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(columns+1, acc_values_mean_arr, 'bo', label='Training accuracy')\n",
    "plt.plot(columns+1, val_acc_values_mean_arr, 'b', label='Validation accuracy')\n",
    "plt.title('Mean training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f\"{path_to_CNN}\\\\mean_accuracy.jpg\")\n",
    "\n",
    "print(\"Mean final loss: {0:.4f}, mean final accuracy: {1:.4f}\".format(final_loss_mean, final_acc_mean))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PSO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def accur (accuracy_target, accuracy_test):\n",
    "    difference = (accuracy_target - accuracy_test)\n",
    "    return difference\n",
    "\n",
    "swarm_size = 2\n",
    "dim = 2  # Dimension of X\n",
    "epsilon = 1.0\n",
    "options = {'c1': 1.5, 'c2': 1.5, 'w': 0.5}\n",
    "\n",
    "CNN_constraints = (np.array([1, 1], dtype=np.int32),\n",
    "                   np.array([8, 5], dtype=np.int32))\n",
    "\n",
    "def fitness_func(X):\n",
    "    n_particles = X.shape[0]  # number of particles\n",
    "    accuracy_target = 1\n",
    "    accur = [(CNN(X[i][0], X[i][1])[3], accuracy_target) for i in range(n_particles)]\n",
    "    return np.array(accur)\n",
    "\n",
    "# Call an instance of PSO\n",
    "optimizerPSO = ps.single.GlobalBestPSO(n_particles=swarm_size,\n",
    "                                       dimensions=dim,\n",
    "                                       options=options,\n",
    "                                       bounds=CNN_constraints)\n",
    "\n",
    "# Perform optimization\n",
    "final_best_cost, final_best_pos = optimizerPSO.optimize(fitness_func, iters=5)\n",
    "\n",
    "print('final_best_cost     final_best_pos\\n'\n",
    "      '{},                 {}'.format(final_best_cost, final_best_pos))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
